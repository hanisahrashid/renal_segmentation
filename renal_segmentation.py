# -*- coding: utf-8 -*-
"""Renal_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r5KXvJfFiSwjorHqwswcpugEDpUUcjlJ
"""

# mount google drive locally
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
os.chdir('/content/drive/My Drive/renal_data')
import pickle
import random
import fnmatch
import numpy as np
import pandas as pd
#from nibabel.testing import data_path
import nibabel as nib
import matplotlib.pyplot as plt
# %matplotlib inline
from natsort import natsorted
from tabulate import tabulate
from skimage.transform import resize
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from scipy import ndimage
from datetime import datetime

import tensorflow as tf
#from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, Activation, Flatten, Dropout, Dense, Conv2DTranspose
from keras.layers.merge import concatenate
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""#Utils"""

def get_data_path(file_dir, pattern):
  file_name = natsorted(os.listdir(file_dir))
  file_list = []
  for file in file_name:
    if fnmatch.fnmatch(file, pattern):
      file_list.append(os.path.join(file_dir, file))
  return file_list


def load_data(path):
  image = nib.load(path)
  image = image.get_fdata()
  if (len(image.shape) > 3):
    image = np.squeeze(image, axis=3)
  image = resize(image, (256, 256, image.shape[2]))
  return image


def fix_position(image):
  image = np.flipud(image)
  image = np.fliplr(image)
  return image


def plot_image(image, mask):
  idx = random.sample(range(0,len(image)-1), 10)
  for i in idx:
    fig, axes = plt.subplots(1, 2,figsize=(20, 20))
    axes[0].imshow(np.swapaxes(image[i], 0, 1), 'gray')
    axes[1].imshow(np.swapaxes(mask[i], 0, 1), 'gray')
    plt.show


def plot_predict(image, mask, pred_mask):
  for i in range(len(image)):
    fig, axes = plt.subplots(1, 3,figsize=(20, 20))
    axes[0].imshow(np.swapaxes(image[i], 0, 1), 'gray')
    axes[1].imshow(np.swapaxes(mask[i], 0, 1), 'gray')
    axes[2].imshow(np.swapaxes(pred_mask[i], 0, 1), 'gray')
    plt.show

def rescale(data):
  black = data.mean() - 0.5 * data.std()
  if black < data.min():
      black = data.min()
  white = data.mean() + 4 * data.std()
  if white > data.max():
      white = data.max()
  data = np.clip(data, black, white) - black
  data = 255.0 * (data / (white - black))
  return data

def get_numpy_data(image_list, mask_list, input_size=256):
  image = []
  mask = []
  for image_path, mask_path in zip(image_list, mask_list):
    print(image_path)
    mri_image = load_data(image_path)
    mri_image = fix_position(mri_image)
    mri_image = rescale(mri_image)
    mri_mask = load_data(mask_path)
    mri_mask = fix_position(mri_mask)

    if mri_image.shape[2] > mri_mask.shape[2]:
      mri_image = np.delete(mri_image, mri_image.shape[2]-1, axis=2)
    if mri_mask.shape[2] > mri_image.shape[2]:
      mri_mask = np.delete(mri_mask, mri_mask.shape[2]-1, axis=2)
    print(mri_image.shape)
    print(mri_mask.shape)
    for n in range(mri_image.shape[2]):
      image_2d = np.reshape(mri_image[:,:,n], (mri_image.shape[0], mri_image.shape[1]))
      mask_2d = np.reshape(mri_mask[:,:,n], (mri_mask.shape[0], mri_mask.shape[1]))
      image.append(image_2d)
      mask.append(mask_2d)

  image_data = np.ndarray((len(image), input_size, input_size), dtype='int32')
  mask_data = np.ndarray((len(mask), input_size, input_size), dtype='float32')

  for index, img in enumerate(image):
    image_data[index, :, :] = img
      
  for index, img in enumerate(mask):
    mask_data[index, :, :] = img

  return image_data, mask_data

class SegmentationStatistics:

  def __init__(self, prediction, truth):
    #if type(prediction) == nib.nifti1.Nifti1Image:
    #  prediction = prediction.get_fdata()
    #if type(truth) == nib.nifti1.Nifti1Image:
    #  truth = truth.get_fdata()
    self.prediction = prediction.astype(bool)
    self.truth = truth.astype(bool)
    self.dice = self.__dice__()
    self.jaccard = self.__jaccard__()
    self.sensitivity = self.__sensitivity__()
    self.specificity = self.__specificity__()
    self.precision = self.__precision__()
    self.accuracy = self.__accuracy__()
    self.dict = self.to_dict()
    self.df = self.to_df()

  def to_dict(self):
    return {'dice': self.dice,
            'jaccard': self.jaccard, 
            'sensitivity': self.sensitivity,
            'specificity': self.specificity,
            'precision': self.precision, 
            'accuracy': self.accuracy}

  def to_df(self):
    df = pd.DataFrame.from_dict(self.dict, orient='index', columns=['Score'])
    df['Metric'] = ['Dice', 'Jaccard', 'Sensitivity', 'Specificity', 'Precision', 'Accuracy']
    df = df[['Metric', 'Score']]
    return df

  def print_table(self):
    print(tabulate(self.df[['Metric', 'Score']], headers=['Metric', 'Score'], tablefmt='github', showindex=False))

  def __dice__(self):
    return np.sum(self.prediction[self.truth==1]) * 2.0 / (np.sum(self.prediction) + np.sum(self.truth))

  def __jaccard__(self):
    return np.sum(self.prediction[self.truth==1]) / (np.sum(self.prediction[self.truth==1]) + np.sum(self.prediction!=self.truth))

  def __sensitivity__(self):
    return np.sum(self.prediction[self.truth==1])/(np.sum(self.prediction[self.truth==1])+np.sum((self.truth==1) & (self.prediction==0)))

  def __specificity__(self):
    return np.sum((self.truth==0) & (self.prediction==0))/(np.sum((self.truth==0) & (self.prediction==0))+np.sum((self.truth==0) & (self.prediction==1)))

  def __precision__(self):
    return np.sum(self.prediction[self.truth==1])/(np.sum(self.prediction[self.truth==1]) + np.sum((self.truth==0) & (self.prediction==1)))

  def __accuracy__(self):
    return (np.sum(self.prediction[self.truth==1]) + np.sum((self.truth==0) & (self.prediction==0))) / self.truth.size

"""#Data preparation"""

folder_dir = './train'
folder_pattern = "*_*"
train_dir = get_data_path(folder_dir, folder_pattern)

image_train = []
mask_train = []

for image in train_dir:
  image_pattern = "*.nii*"
  image_path = get_data_path(image, image_pattern)
  for i in image_path:
    if "mask" in i:
      mask_train.append(i)
    elif "T2" in i:
      image_train.append(i)
      print(image_train.index(i), i)
    else:
      print("Unclasified data exists")

print("MRI training data: ", len(image_train))
print("Mask data: ", len(mask_train))

folder_dir = './val'
folder_pattern = "*_*"
val_dir = get_data_path(folder_dir, folder_pattern)

image_val = []
mask_val = []

for image in val_dir:
  image_pattern = "*.nii*"
  image_path = get_data_path(image, image_pattern)
  for i in image_path:
    if "mask" in i:
      mask_val.append(i)
    elif "T2" in i:
      image_val.append(i)
      print(image_val.index(i), i)
    else:
      print("Unclasified data exists")

print("MRI validation data: ", len(image_val))
print("Mask data: ", len(mask_val))

folder_dir = './test'
folder_pattern = "*_*"
test_dir = get_data_path(folder_dir, folder_pattern)
len(test_dir)

image_test = []
mask_test = []

for image in test_dir:
  image_pattern = "*.nii*"
  image_path = get_data_path(image, image_pattern)
  for i in image_path:
    if "mask" in i:
      mask_test.append(i)
    elif "T2" in i:
      image_test.append(i)
      print(image_test.index(i), i)
    else:
      print("Unclasified data exists")

print("MRI test data: ", len(image_test))
print("Mask data: ", len(mask_test))

print("Train data: ")
x_train, y_train = get_numpy_data(image_train, mask_train)
print("Val data: ")
x_val, y_val = get_numpy_data(image_val, mask_val)
print("Test data: ")
x_test, y_test = get_numpy_data(image_test, mask_test)

#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42)
print(x_train.shape, y_train.shape)
print(x_val.shape, y_val.shape)
print(x_test.shape, y_test.shape)

plot_image(x_train, y_train)

plot_image(x_val, y_val)

plot_image(x_test, y_test)

"""#Data augmentation"""

def rotate(image, mask):
  angle = random.randint(-5,5)
  rotated_image = ndimage.rotate(image, angle, reshape=False, order=0)
  rotated_mask = ndimage.rotate(mask, angle, reshape=False, order=0)
  return rotated_image, rotated_mask

def flip(image, mask):
  flipped_image = np.flipud(image)
  flipped_mask = np.flipud(mask)
  return flipped_image, flipped_mask

def translation(image, mask):
  x= random.randint(-20,20)
  y = random.randint(-20,20)
  translate_image = ndimage.shift(image, (x, y), order=0)
  translate_mask = ndimage.shift(mask, (x, y), order=0)
  return translate_image, translate_mask

def apply_aug(image, mask):
  if np.random.rand() < 0.5:
    image, mask = rotate(image, mask)
  if np.random.rand() < 0.5:
    image, mask = flip(image, mask)
  if np.random.rand() < 1.0:
    image, mask = translation(image, mask)
  
  return image, mask

def image_generator(x_image, y_mask, batch_size, is_training):
  while True:
    batch_image = []
    batch_mask = []

    for i in range(batch_size):
      random_index = random.randint(0, len(x_image)-1)
      img = x_image[random_index]
      msk = y_mask[random_index]
      if is_training:
        img, msk = apply_aug(img, msk)
        #plt.imshow(np.swapaxes(img, 0, 1), 'gray')
        #plt.imshow(np.swapaxes(msk, 0, 1), 'gray', alpha=0.5)
        #plt.show()

      batch_image.append(img)
      batch_mask.append(msk)

    yield (np.asarray(batch_image), np.asarray(batch_mask))

next(image_generator(x_train, y_train, 32, True))

"""#Model"""

# Build U-Net model

def u_net(height, width, channel):
  
  inputs = Input(shape = (height, width, channel))

  conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
  conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
  batch_norm1 = BatchNormalization()(conv1)
  pool1 = MaxPooling2D(pool_size=(2, 2))(batch_norm1)

  conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
  conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
  batch_norm2 = BatchNormalization()(conv2)
  pool2 = MaxPooling2D(pool_size=(2, 2))(batch_norm2)

  conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
  conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
  batch_norm3 = BatchNormalization()(conv3)
  pool3 = MaxPooling2D(pool_size=(2, 2))(batch_norm3)
  #drop3 = Dropout(0.2)(pool3)

  conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
  conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
  batch_norm4 = BatchNormalization()(conv4)
  pool4 = MaxPooling2D(pool_size=(2, 2))(batch_norm4)
  #drop4 = Dropout(0.2)(pool4)

  conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
  conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

  up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)
  conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
  conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

  up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)
  conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
  conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

  up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)
  conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
  conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

  up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)
  conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
  conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

  conv10 = Conv2D(channel, (1, 1), activation='sigmoid')(conv9)

  model = Model(inputs = [inputs], outputs = [conv10])


  return model

def dice_coef(y_true, y_pred):
    smooth = 1.0
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_coef_loss(y_true, y_pred):
    return -dice_coef(y_true, y_pred)

"""#Process"""

batch_size = 2**3
nb_epoch = 100
input_size = 256
n_channel = 1
output_path = "./model/"
adam = Adam(learning_rate=0.0001)
model = u_net(input_size, input_size, n_channel)
model.compile(optimizer=adam, loss=dice_coef_loss, metrics=[dice_coef])
model.summary()

callback_checkpoint = [EarlyStopping(patience=10),
                       ModelCheckpoint(filepath=output_path + datetime.now().strftime("%m_%d_%H_%M_") + "model.{epoch:02d}-{val_dice_coef:.2f}.hdf5",
                                       monitor='val_loss', verbose=1, save_best_only=True, mode='min')]

history = model.fit(x_train, y_train,
                       epochs=nb_epoch,
                       steps_per_epoch=len(x_train)//batch_size,
                       #batch_size=batch_size,
                       verbose=1,
                       shuffle=True,
                       validation_data=(x_val, y_val),
                       callbacks=callback_checkpoint
                       )

model.save(output_path + datetime.now().strftime("%m_%d_%H_%M_") + "model.hdf5")

history_path = output_path + datetime.now().strftime("%m_%d_%H_%M_") + "history.pickle"
with open(history_path, 'wb') as f:
    pickle.dump(history.history, f, pickle.HIGHEST_PROTOCOL)

"""# Evaluation"""

history_path = output_path + "11_14_06_54_history.pickle"
with open(history_path, 'rb') as f:
  history = pickle.load(f)

plt.figure()
plt.plot(history['dice_coef'], label='Training score')
plt.plot(history['val_dice_coef'], label='Validation score')
plt.xlabel('Epoch')
plt.ylabel('Dice Score')
plt.legend()
plt.grid()
plt.show()

plt.figure()
plt.plot(history['loss'], label='Training loss')
plt.plot(history['val_loss'], label='Validation loss')
plt.xlabel('Epoch')
plt.ylabel('Dice Loss')
plt.legend()
plt.grid()
plt.show()

model = load_model("./model/11_14_06_48_model.51-0.92.hdf5", custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})

predict = model.predict(x_test)
print(np.shape(predict))

fixed_predict = np.squeeze(predict)
tkv_stats = SegmentationStatistics(fixed_predict, y_test)
print(tkv_stats.print_table())

plot_predict(x_test, y_test, fixed_predict)

